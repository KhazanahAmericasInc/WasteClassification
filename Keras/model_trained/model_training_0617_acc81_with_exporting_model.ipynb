{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ8yFsv3JUGo",
        "colab_type": "code",
        "outputId": "2752db4c-8aa9-4c09-d193-e8ba57a417bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import h5py\n",
        "import math\n",
        "#import data_preprocessing as dp\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from urllib.request import urlopen,urlretrieve\n",
        "from PIL import Image\n",
        "from tqdm import tqdm_notebook\n",
        "from sklearn.utils import shuffle\n",
        "import cv2\n",
        "#from resnet_utils import *\n",
        "\n",
        "from keras.models import load_model\n",
        "from sklearn.datasets import load_files   \n",
        "from keras.utils import np_utils\n",
        "from glob import glob\n",
        "from keras import applications\n",
        "from keras.preprocessing.image import ImageDataGenerator \n",
        "from keras import optimizers\n",
        "from keras.models import Sequential,Model,load_model\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D,GlobalAveragePooling2D\n",
        "from keras.callbacks import TensorBoard,ReduceLROnPlateau,ModelCheckpoint\n",
        "\n",
        "\n",
        "################# prepare data #################\n",
        "'''\n",
        "load dataset\n",
        "normalize\n",
        "dummy variables (encode y)\n",
        "constants\n",
        "\n",
        "'''\n",
        "\n",
        "################ model training #################\n",
        "'''\n",
        "build up architecture\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "'''\n",
        "load dataset from folder\n",
        "'''\n",
        "def load_dataset():\n",
        "    train_dataset = h5py.File('./training-data-0528.h5', \"r\")\n",
        "    train_set_x_orig = np.array(train_dataset['X'][:])\n",
        "    train_set_y_orig = np.array(train_dataset[\"y\"][:])\n",
        "\n",
        "    test_dataset = h5py.File('./testing-data-0528.h5', \"r\")\n",
        "    test_set_x_orig = np.array(test_dataset['X'][:])\n",
        "    test_set_y_orig = np.array(test_dataset[\"y\"][:])\n",
        "    \n",
        "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
        "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
        "\n",
        "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig\n",
        "\n",
        "\n",
        "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
        "    \"\"\"\n",
        "    Creates a list of random minibatches from (X, Y)\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input data, of shape (input size, number of examples) (m, Hi, Wi, Ci)\n",
        "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples) (m, n_y)\n",
        "    mini_batch_size - size of the mini-batches, integer\n",
        "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
        "    \n",
        "    Returns:\n",
        "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[0]                  # number of training examples\n",
        "    mini_batches = []\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    # Step 1: Shuffle (X, Y)\n",
        "    permutation = list(np.random.permutation(m))\n",
        "    shuffled_X = X[permutation,:,:,:]\n",
        "    shuffled_Y = Y[permutation,:]\n",
        "\n",
        "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
        "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
        "    for k in range(0, num_complete_minibatches):\n",
        "        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:,:]\n",
        "        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    # Handling the end case (last mini-batch < mini_batch_size)\n",
        "    if m % mini_batch_size != 0:\n",
        "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m,:,:,:]\n",
        "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m,:]\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    return mini_batches\n",
        "\n",
        "\n",
        "def convert_to_one_hot(Y, C):\n",
        "    Y = np.eye(C)[Y.reshape(-1)].T\n",
        "    return Y\n",
        "\n",
        "\n",
        "def forward_propagation_for_predict(X, parameters):\n",
        "    \"\"\"\n",
        "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
        "                  the shapes are given in initialize_parameters\n",
        "    Returns:\n",
        "    Z3 -- the output of the last LINEAR unit\n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieve the parameters from the dictionary \"parameters\" \n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    W3 = parameters['W3']\n",
        "    b3 = parameters['b3'] \n",
        "                                                           # Numpy Equivalents:\n",
        "    Z1 = tf.add(tf.matmul(W1, X), b1)                      # Z1 = np.dot(W1, X) + b1\n",
        "    A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n",
        "    Z2 = tf.add(tf.matmul(W2, A1), b2)                     # Z2 = np.dot(W2, a1) + b2\n",
        "    A2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n",
        "    Z3 = tf.add(tf.matmul(W3, A2), b3)                     # Z3 = np.dot(W3,Z2) + b3\n",
        "    \n",
        "    return Z3\n",
        "\n",
        "def predict(X, parameters):\n",
        "    \n",
        "    W1 = tf.convert_to_tensor(parameters[\"W1\"])\n",
        "    b1 = tf.convert_to_tensor(parameters[\"b1\"])\n",
        "    W2 = tf.convert_to_tensor(parameters[\"W2\"])\n",
        "    b2 = tf.convert_to_tensor(parameters[\"b2\"])\n",
        "    W3 = tf.convert_to_tensor(parameters[\"W3\"])\n",
        "    b3 = tf.convert_to_tensor(parameters[\"b3\"])\n",
        "    \n",
        "    params = {\"W1\": W1,\n",
        "              \"b1\": b1,\n",
        "              \"W2\": W2,\n",
        "              \"b2\": b2,\n",
        "              \"W3\": W3,\n",
        "              \"b3\": b3}\n",
        "    \n",
        "    x = tf.placeholder(name=\"x\", dtype=tf.float32, shape=(12288, 1))\n",
        "    var = tf.get_variable(\"weights\", dtype=tf.float32, shape=(12288, 1))\n",
        "    val = img + var\n",
        "    out = tf.identity(val, name=\"out\")\n",
        "    \n",
        "    z3 = forward_propagation_for_predict(x, params)\n",
        "    p = tf.argmax(z3)\n",
        "    \n",
        "    sess = tf.Session()\n",
        "    prediction = sess.run(p, feed_dict = {x: X})\n",
        "    \n",
        "    converter = tf.lite.TFLiteConverter.from_session(sess, [x], [out])\n",
        "    tflite_model = converter.convert()\n",
        "    open(\"converted_model.tflite\", \"wb\").write(tflite_model)\n",
        "        \n",
        "    return prediction\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "normalize the image data\n",
        "'''\n",
        "def normalize():\n",
        "    # Normalize image vectors\n",
        "    X_train = X_train_orig/255.\n",
        "    X_test = X_test_orig/255.\n",
        "    return X_train, X_test\n",
        "    \n",
        "'''\n",
        "one hot encoding y labels\n",
        "'''\n",
        "def encode_y():\n",
        "    Y_train = convert_to_one_hot(Y_train_orig, 3).T\n",
        "    Y_test = convert_to_one_hot(Y_test_orig, 3).T\n",
        "    return Y_train, Y_test\n",
        "    \n",
        "def check_shape():\n",
        "    print (\"number of training examples = \" + str(X_train.shape[0]))\n",
        "    print (\"number of test examples = \" + str(X_test.shape[0]))\n",
        "    print (\"X_train shape: \" + str(X_train.shape))\n",
        "    print (\"Y_train shape: \" + str(Y_train.shape))\n",
        "    print (\"X_test shape: \" + str(X_test.shape))\n",
        "    print (\"Y_test shape: \" + str(Y_test.shape))\n",
        "    \n",
        "    \n",
        "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig = load_dataset()\n",
        "normalize()\n",
        "\n",
        "X_train, X_test = normalize()\n",
        "Y_train, Y_test = encode_y()\n",
        "\n",
        "img_height,img_width = 64,64 \n",
        "num_classes = 3\n",
        "\n",
        "base_model = applications.resnet50.ResNet50(weights= None, include_top=False, input_shape= (img_height,img_width,3))    \n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.7)(x)\n",
        "predictions = Dense(num_classes, activation= 'softmax')(x)\n",
        "model = Model(inputs = base_model.input, outputs = predictions)    \n",
        "    \n",
        "from keras.optimizers import SGD, Adam\n",
        "sgd = SGD(lr=0.001, momentum=0.9, nesterov=False)\n",
        "# adam = Adam(lr=0.0001)\n",
        "model.compile(optimizer= sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0617 17:53:06.860518 140308561700736 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0617 17:53:06.892981 140308561700736 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0617 17:53:06.902336 140308561700736 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "W0617 17:53:06.936290 140308561700736 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0617 17:53:06.937262 140308561700736 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0617 17:53:09.532986 140308561700736 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0617 17:53:09.604743 140308561700736 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
            "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n",
            "W0617 17:53:14.162336 140308561700736 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0617 17:53:14.163228 140308561700736 nn_ops.py:4224] Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "W0617 17:53:14.209209 140308561700736 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VzBMHp_QfsO",
        "colab_type": "code",
        "outputId": "213f357d-5dcb-4174-e4d5-bf8b70ce58dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6905
        }
      },
      "source": [
        "history = model.fit(X_train, Y_train , validation_split=0.2, epochs = 200, batch_size = 64)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0617 17:53:29.655621 140308561700736 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1618 samples, validate on 405 samples\n",
            "Epoch 1/200\n",
            "1618/1618 [==============================] - 16s 10ms/step - loss: 1.6509 - acc: 0.5667 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
            "Epoch 2/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 1.2988 - acc: 0.5989 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
            "Epoch 3/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 1.1203 - acc: 0.6224 - val_loss: 14.2407 - val_acc: 0.0198\n",
            "Epoch 4/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 1.0060 - acc: 0.6514 - val_loss: 11.8548 - val_acc: 0.0272\n",
            "Epoch 5/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.8827 - acc: 0.6749 - val_loss: 6.9360 - val_acc: 0.2123\n",
            "Epoch 6/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.8289 - acc: 0.7114 - val_loss: 4.8961 - val_acc: 0.3012\n",
            "Epoch 7/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.6924 - acc: 0.7454 - val_loss: 2.7913 - val_acc: 0.4000\n",
            "Epoch 8/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.6597 - acc: 0.7849 - val_loss: 2.6322 - val_acc: 0.5037\n",
            "Epoch 9/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.5574 - acc: 0.8183 - val_loss: 2.6516 - val_acc: 0.4420\n",
            "Epoch 10/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.4640 - acc: 0.8430 - val_loss: 1.2350 - val_acc: 0.6765\n",
            "Epoch 11/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.4283 - acc: 0.8640 - val_loss: 1.1276 - val_acc: 0.7037\n",
            "Epoch 12/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.3618 - acc: 0.8925 - val_loss: 0.9731 - val_acc: 0.7481\n",
            "Epoch 13/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.3522 - acc: 0.9042 - val_loss: 1.2417 - val_acc: 0.6938\n",
            "Epoch 14/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.2570 - acc: 0.9283 - val_loss: 1.6624 - val_acc: 0.6198\n",
            "Epoch 15/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.2576 - acc: 0.9265 - val_loss: 1.0886 - val_acc: 0.7235\n",
            "Epoch 16/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.2231 - acc: 0.9308 - val_loss: 1.9713 - val_acc: 0.6642\n",
            "Epoch 17/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.1671 - acc: 0.9561 - val_loss: 2.0594 - val_acc: 0.6469\n",
            "Epoch 18/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.1533 - acc: 0.9592 - val_loss: 1.1457 - val_acc: 0.7407\n",
            "Epoch 19/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.1708 - acc: 0.9660 - val_loss: 2.2469 - val_acc: 0.6395\n",
            "Epoch 20/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.1272 - acc: 0.9710 - val_loss: 1.4758 - val_acc: 0.6914\n",
            "Epoch 21/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.1023 - acc: 0.9759 - val_loss: 1.8216 - val_acc: 0.6667\n",
            "Epoch 22/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0968 - acc: 0.9827 - val_loss: 2.1641 - val_acc: 0.6272\n",
            "Epoch 23/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0961 - acc: 0.9784 - val_loss: 1.9618 - val_acc: 0.6346\n",
            "Epoch 24/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0890 - acc: 0.9852 - val_loss: 2.2114 - val_acc: 0.6247\n",
            "Epoch 25/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.1240 - acc: 0.9734 - val_loss: 1.8241 - val_acc: 0.6716\n",
            "Epoch 26/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.1268 - acc: 0.9778 - val_loss: 2.0443 - val_acc: 0.6370\n",
            "Epoch 27/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.1188 - acc: 0.9790 - val_loss: 1.7361 - val_acc: 0.6765\n",
            "Epoch 28/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.1093 - acc: 0.9771 - val_loss: 1.5861 - val_acc: 0.6938\n",
            "Epoch 29/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.1109 - acc: 0.9790 - val_loss: 1.9129 - val_acc: 0.6420\n",
            "Epoch 30/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.1008 - acc: 0.9765 - val_loss: 1.9486 - val_acc: 0.6321\n",
            "Epoch 31/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0891 - acc: 0.9833 - val_loss: 1.8044 - val_acc: 0.6716\n",
            "Epoch 32/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0970 - acc: 0.9784 - val_loss: 1.4419 - val_acc: 0.7111\n",
            "Epoch 33/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.1453 - acc: 0.9666 - val_loss: 2.9117 - val_acc: 0.5531\n",
            "Epoch 34/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.1515 - acc: 0.9672 - val_loss: 2.6297 - val_acc: 0.5457\n",
            "Epoch 35/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.1495 - acc: 0.9635 - val_loss: 2.4408 - val_acc: 0.5753\n",
            "Epoch 36/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.1479 - acc: 0.9629 - val_loss: 1.9991 - val_acc: 0.6568\n",
            "Epoch 37/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.1229 - acc: 0.9691 - val_loss: 1.9944 - val_acc: 0.6519\n",
            "Epoch 38/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.1080 - acc: 0.9827 - val_loss: 0.8360 - val_acc: 0.7926\n",
            "Epoch 39/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.3054 - acc: 0.9530 - val_loss: 2.9305 - val_acc: 0.5037\n",
            "Epoch 40/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.1332 - acc: 0.9666 - val_loss: 1.7708 - val_acc: 0.6642\n",
            "Epoch 41/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0862 - acc: 0.9802 - val_loss: 1.1462 - val_acc: 0.7580\n",
            "Epoch 42/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0978 - acc: 0.9759 - val_loss: 1.1615 - val_acc: 0.7556\n",
            "Epoch 43/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.1012 - acc: 0.9778 - val_loss: 1.2159 - val_acc: 0.7531\n",
            "Epoch 44/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0719 - acc: 0.9821 - val_loss: 1.0685 - val_acc: 0.7901\n",
            "Epoch 45/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.1219 - acc: 0.9728 - val_loss: 1.7002 - val_acc: 0.6469\n",
            "Epoch 46/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.1042 - acc: 0.9747 - val_loss: 1.4860 - val_acc: 0.6938\n",
            "Epoch 47/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0747 - acc: 0.9864 - val_loss: 1.4888 - val_acc: 0.7037\n",
            "Epoch 48/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0794 - acc: 0.9815 - val_loss: 1.8640 - val_acc: 0.6420\n",
            "Epoch 49/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0912 - acc: 0.9839 - val_loss: 1.3597 - val_acc: 0.7284\n",
            "Epoch 50/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0640 - acc: 0.9870 - val_loss: 1.9763 - val_acc: 0.6370\n",
            "Epoch 51/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0875 - acc: 0.9740 - val_loss: 3.8214 - val_acc: 0.4716\n",
            "Epoch 52/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0867 - acc: 0.9778 - val_loss: 3.0351 - val_acc: 0.5556\n",
            "Epoch 53/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0774 - acc: 0.9839 - val_loss: 2.3825 - val_acc: 0.6247\n",
            "Epoch 54/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0701 - acc: 0.9839 - val_loss: 2.7194 - val_acc: 0.6123\n",
            "Epoch 55/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0681 - acc: 0.9858 - val_loss: 1.9263 - val_acc: 0.6741\n",
            "Epoch 56/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0537 - acc: 0.9883 - val_loss: 1.7470 - val_acc: 0.6914\n",
            "Epoch 57/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0480 - acc: 0.9895 - val_loss: 2.0115 - val_acc: 0.6617\n",
            "Epoch 58/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0332 - acc: 0.9932 - val_loss: 1.7223 - val_acc: 0.6889\n",
            "Epoch 59/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0344 - acc: 0.9920 - val_loss: 1.6154 - val_acc: 0.7160\n",
            "Epoch 60/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0524 - acc: 0.9852 - val_loss: 2.1514 - val_acc: 0.6247\n",
            "Epoch 61/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0433 - acc: 0.9901 - val_loss: 1.4852 - val_acc: 0.7309\n",
            "Epoch 62/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0368 - acc: 0.9932 - val_loss: 1.4329 - val_acc: 0.7185\n",
            "Epoch 63/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0262 - acc: 0.9938 - val_loss: 1.8552 - val_acc: 0.6519\n",
            "Epoch 64/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0314 - acc: 0.9938 - val_loss: 1.6839 - val_acc: 0.6963\n",
            "Epoch 65/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0363 - acc: 0.9907 - val_loss: 1.8308 - val_acc: 0.6741\n",
            "Epoch 66/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0205 - acc: 0.9951 - val_loss: 1.5396 - val_acc: 0.7210\n",
            "Epoch 67/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0168 - acc: 0.9975 - val_loss: 1.5648 - val_acc: 0.7284\n",
            "Epoch 68/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0612 - acc: 0.9889 - val_loss: 2.1941 - val_acc: 0.6222\n",
            "Epoch 69/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0333 - acc: 0.9913 - val_loss: 2.1041 - val_acc: 0.6543\n",
            "Epoch 70/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0434 - acc: 0.9932 - val_loss: 1.8384 - val_acc: 0.7012\n",
            "Epoch 71/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0572 - acc: 0.9864 - val_loss: 1.9824 - val_acc: 0.6815\n",
            "Epoch 72/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0226 - acc: 0.9963 - val_loss: 1.1822 - val_acc: 0.7877\n",
            "Epoch 73/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0258 - acc: 0.9932 - val_loss: 1.3759 - val_acc: 0.7506\n",
            "Epoch 74/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0264 - acc: 0.9969 - val_loss: 1.1193 - val_acc: 0.7877\n",
            "Epoch 75/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0415 - acc: 0.9932 - val_loss: 1.4013 - val_acc: 0.7506\n",
            "Epoch 76/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0268 - acc: 0.9944 - val_loss: 1.7829 - val_acc: 0.7012\n",
            "Epoch 77/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0348 - acc: 0.9932 - val_loss: 2.0371 - val_acc: 0.6889\n",
            "Epoch 78/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0428 - acc: 0.9938 - val_loss: 1.8625 - val_acc: 0.7062\n",
            "Epoch 79/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0353 - acc: 0.9926 - val_loss: 1.6274 - val_acc: 0.7210\n",
            "Epoch 80/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0379 - acc: 0.9926 - val_loss: 1.7887 - val_acc: 0.6963\n",
            "Epoch 81/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0675 - acc: 0.9852 - val_loss: 1.6813 - val_acc: 0.7012\n",
            "Epoch 82/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0519 - acc: 0.9833 - val_loss: 1.7340 - val_acc: 0.6963\n",
            "Epoch 83/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0293 - acc: 0.9920 - val_loss: 2.1253 - val_acc: 0.6272\n",
            "Epoch 84/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0293 - acc: 0.9920 - val_loss: 2.5726 - val_acc: 0.6148\n",
            "Epoch 85/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0292 - acc: 0.9901 - val_loss: 2.8001 - val_acc: 0.6049\n",
            "Epoch 86/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0396 - acc: 0.9907 - val_loss: 2.5704 - val_acc: 0.6370\n",
            "Epoch 87/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0497 - acc: 0.9883 - val_loss: 1.3945 - val_acc: 0.7457\n",
            "Epoch 88/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0446 - acc: 0.9901 - val_loss: 1.8198 - val_acc: 0.7136\n",
            "Epoch 89/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0398 - acc: 0.9913 - val_loss: 2.2435 - val_acc: 0.6543\n",
            "Epoch 90/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0352 - acc: 0.9901 - val_loss: 2.4098 - val_acc: 0.6395\n",
            "Epoch 91/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0282 - acc: 0.9932 - val_loss: 2.5641 - val_acc: 0.6173\n",
            "Epoch 92/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0317 - acc: 0.9913 - val_loss: 2.2875 - val_acc: 0.6642\n",
            "Epoch 93/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0234 - acc: 0.9944 - val_loss: 1.8238 - val_acc: 0.7086\n",
            "Epoch 94/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0315 - acc: 0.9895 - val_loss: 1.6351 - val_acc: 0.7284\n",
            "Epoch 95/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0229 - acc: 0.9957 - val_loss: 1.5956 - val_acc: 0.7111\n",
            "Epoch 96/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0168 - acc: 0.9981 - val_loss: 1.9438 - val_acc: 0.6864\n",
            "Epoch 97/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0235 - acc: 0.9975 - val_loss: 1.9429 - val_acc: 0.6840\n",
            "Epoch 98/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0309 - acc: 0.9957 - val_loss: 1.8132 - val_acc: 0.7185\n",
            "Epoch 99/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0200 - acc: 0.9975 - val_loss: 1.8583 - val_acc: 0.7037\n",
            "Epoch 100/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0126 - acc: 0.9988 - val_loss: 1.6920 - val_acc: 0.7235\n",
            "Epoch 101/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0136 - acc: 0.9981 - val_loss: 1.7920 - val_acc: 0.7136\n",
            "Epoch 102/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0223 - acc: 0.9938 - val_loss: 1.7810 - val_acc: 0.7210\n",
            "Epoch 103/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0310 - acc: 0.9944 - val_loss: 1.8605 - val_acc: 0.7210\n",
            "Epoch 104/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0213 - acc: 0.9957 - val_loss: 1.6400 - val_acc: 0.7284\n",
            "Epoch 105/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0323 - acc: 0.9920 - val_loss: 2.2153 - val_acc: 0.6840\n",
            "Epoch 106/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0243 - acc: 0.9938 - val_loss: 1.7809 - val_acc: 0.7185\n",
            "Epoch 107/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0315 - acc: 0.9920 - val_loss: 1.8120 - val_acc: 0.7185\n",
            "Epoch 108/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0555 - acc: 0.9858 - val_loss: 3.0785 - val_acc: 0.5728\n",
            "Epoch 109/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0254 - acc: 0.9957 - val_loss: 2.3241 - val_acc: 0.6568\n",
            "Epoch 110/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0281 - acc: 0.9932 - val_loss: 2.4476 - val_acc: 0.6198\n",
            "Epoch 111/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0284 - acc: 0.9932 - val_loss: 2.7330 - val_acc: 0.6074\n",
            "Epoch 112/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0300 - acc: 0.9944 - val_loss: 2.5728 - val_acc: 0.6222\n",
            "Epoch 113/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0217 - acc: 0.9969 - val_loss: 2.2146 - val_acc: 0.6469\n",
            "Epoch 114/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0339 - acc: 0.9907 - val_loss: 2.1669 - val_acc: 0.6395\n",
            "Epoch 115/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0178 - acc: 0.9969 - val_loss: 2.1066 - val_acc: 0.6617\n",
            "Epoch 116/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0152 - acc: 0.9981 - val_loss: 2.1621 - val_acc: 0.6741\n",
            "Epoch 117/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0152 - acc: 0.9981 - val_loss: 2.1142 - val_acc: 0.6864\n",
            "Epoch 118/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0229 - acc: 0.9988 - val_loss: 2.2624 - val_acc: 0.6741\n",
            "Epoch 119/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0203 - acc: 0.9963 - val_loss: 1.8749 - val_acc: 0.6963\n",
            "Epoch 120/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0163 - acc: 0.9981 - val_loss: 2.1104 - val_acc: 0.6691\n",
            "Epoch 121/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0130 - acc: 0.9988 - val_loss: 2.3605 - val_acc: 0.6519\n",
            "Epoch 122/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0125 - acc: 0.9988 - val_loss: 2.3270 - val_acc: 0.6642\n",
            "Epoch 123/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0141 - acc: 0.9981 - val_loss: 2.5849 - val_acc: 0.6519\n",
            "Epoch 124/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0125 - acc: 0.9981 - val_loss: 2.3165 - val_acc: 0.6519\n",
            "Epoch 125/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0129 - acc: 0.9981 - val_loss: 2.3684 - val_acc: 0.6395\n",
            "Epoch 126/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0152 - acc: 0.9975 - val_loss: 2.3453 - val_acc: 0.6642\n",
            "Epoch 127/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0158 - acc: 0.9975 - val_loss: 2.2261 - val_acc: 0.6667\n",
            "Epoch 128/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0219 - acc: 0.9944 - val_loss: 2.1380 - val_acc: 0.6691\n",
            "Epoch 129/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0221 - acc: 0.9957 - val_loss: 2.6371 - val_acc: 0.6247\n",
            "Epoch 130/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0129 - acc: 0.9994 - val_loss: 2.3150 - val_acc: 0.6543\n",
            "Epoch 131/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0262 - acc: 0.9951 - val_loss: 1.6354 - val_acc: 0.7358\n",
            "Epoch 132/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0230 - acc: 0.9981 - val_loss: 1.9926 - val_acc: 0.6889\n",
            "Epoch 133/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0322 - acc: 0.9957 - val_loss: 2.4027 - val_acc: 0.6494\n",
            "Epoch 134/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0160 - acc: 0.9981 - val_loss: 2.0153 - val_acc: 0.6963\n",
            "Epoch 135/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0125 - acc: 0.9988 - val_loss: 2.0695 - val_acc: 0.7062\n",
            "Epoch 136/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0221 - acc: 0.9951 - val_loss: 1.9570 - val_acc: 0.7111\n",
            "Epoch 137/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0137 - acc: 0.9975 - val_loss: 2.3290 - val_acc: 0.6568\n",
            "Epoch 138/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0197 - acc: 0.9963 - val_loss: 1.9069 - val_acc: 0.7037\n",
            "Epoch 139/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0185 - acc: 0.9944 - val_loss: 1.7270 - val_acc: 0.7309\n",
            "Epoch 140/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0162 - acc: 0.9975 - val_loss: 1.4181 - val_acc: 0.7728\n",
            "Epoch 141/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0131 - acc: 0.9988 - val_loss: 1.5645 - val_acc: 0.7407\n",
            "Epoch 142/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0201 - acc: 0.9951 - val_loss: 1.4324 - val_acc: 0.7457\n",
            "Epoch 143/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0193 - acc: 0.9969 - val_loss: 1.5102 - val_acc: 0.7432\n",
            "Epoch 144/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0296 - acc: 0.9920 - val_loss: 1.7007 - val_acc: 0.7111\n",
            "Epoch 145/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0195 - acc: 0.9963 - val_loss: 2.2290 - val_acc: 0.6716\n",
            "Epoch 146/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0163 - acc: 0.9963 - val_loss: 2.0623 - val_acc: 0.6741\n",
            "Epoch 147/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0180 - acc: 0.9963 - val_loss: 1.8267 - val_acc: 0.6988\n",
            "Epoch 148/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0211 - acc: 0.9944 - val_loss: 2.0250 - val_acc: 0.6593\n",
            "Epoch 149/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0279 - acc: 0.9963 - val_loss: 1.5669 - val_acc: 0.7432\n",
            "Epoch 150/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0181 - acc: 0.9969 - val_loss: 1.6065 - val_acc: 0.7457\n",
            "Epoch 151/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0216 - acc: 0.9981 - val_loss: 1.5120 - val_acc: 0.7679\n",
            "Epoch 152/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0333 - acc: 0.9938 - val_loss: 1.6863 - val_acc: 0.7556\n",
            "Epoch 153/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0222 - acc: 0.9944 - val_loss: 1.8745 - val_acc: 0.7037\n",
            "Epoch 154/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0181 - acc: 0.9969 - val_loss: 1.9578 - val_acc: 0.6938\n",
            "Epoch 155/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0214 - acc: 0.9957 - val_loss: 1.8058 - val_acc: 0.7062\n",
            "Epoch 156/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0150 - acc: 0.9975 - val_loss: 1.7306 - val_acc: 0.7160\n",
            "Epoch 157/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0176 - acc: 0.9969 - val_loss: 1.8057 - val_acc: 0.7210\n",
            "Epoch 158/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0117 - acc: 0.9994 - val_loss: 1.9053 - val_acc: 0.7111\n",
            "Epoch 159/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0131 - acc: 0.9988 - val_loss: 2.0323 - val_acc: 0.7037\n",
            "Epoch 160/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0143 - acc: 0.9969 - val_loss: 1.9639 - val_acc: 0.7062\n",
            "Epoch 161/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0135 - acc: 0.9981 - val_loss: 1.9711 - val_acc: 0.6963\n",
            "Epoch 162/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0113 - acc: 0.9994 - val_loss: 2.1436 - val_acc: 0.6889\n",
            "Epoch 163/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0189 - acc: 0.9963 - val_loss: 2.2093 - val_acc: 0.6765\n",
            "Epoch 164/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0333 - acc: 0.9963 - val_loss: 2.0677 - val_acc: 0.6914\n",
            "Epoch 165/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0184 - acc: 0.9969 - val_loss: 1.7981 - val_acc: 0.7210\n",
            "Epoch 166/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0126 - acc: 0.9988 - val_loss: 1.7454 - val_acc: 0.7210\n",
            "Epoch 167/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0140 - acc: 0.9988 - val_loss: 2.0977 - val_acc: 0.6889\n",
            "Epoch 168/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0184 - acc: 0.9951 - val_loss: 2.1865 - val_acc: 0.6765\n",
            "Epoch 169/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0088 - acc: 0.9969 - val_loss: 3.1012 - val_acc: 0.5901\n",
            "Epoch 170/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0042 - acc: 0.9981 - val_loss: 3.3904 - val_acc: 0.5580\n",
            "Epoch 171/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0138 - acc: 0.9988 - val_loss: 2.9518 - val_acc: 0.6099\n",
            "Epoch 172/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0054 - acc: 0.9994 - val_loss: 2.5195 - val_acc: 0.6519\n",
            "Epoch 173/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0083 - acc: 0.9963 - val_loss: 2.4645 - val_acc: 0.6494\n",
            "Epoch 174/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0073 - acc: 0.9975 - val_loss: 2.0096 - val_acc: 0.6889\n",
            "Epoch 175/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0147 - acc: 0.9969 - val_loss: 2.3358 - val_acc: 0.6617\n",
            "Epoch 176/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0374 - acc: 0.9901 - val_loss: 2.2610 - val_acc: 0.6691\n",
            "Epoch 177/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0197 - acc: 0.9938 - val_loss: 2.1416 - val_acc: 0.6889\n",
            "Epoch 178/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0096 - acc: 0.9963 - val_loss: 1.9952 - val_acc: 0.6864\n",
            "Epoch 179/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0205 - acc: 0.9938 - val_loss: 1.6303 - val_acc: 0.7383\n",
            "Epoch 180/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0074 - acc: 0.9957 - val_loss: 1.5249 - val_acc: 0.7506\n",
            "Epoch 181/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0219 - acc: 0.9920 - val_loss: 2.3388 - val_acc: 0.6469\n",
            "Epoch 182/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0063 - acc: 0.9981 - val_loss: 2.4313 - val_acc: 0.6593\n",
            "Epoch 183/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0099 - acc: 0.9963 - val_loss: 2.0104 - val_acc: 0.7086\n",
            "Epoch 184/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0159 - acc: 0.9944 - val_loss: 1.8342 - val_acc: 0.7432\n",
            "Epoch 185/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0204 - acc: 0.9907 - val_loss: 2.2453 - val_acc: 0.6790\n",
            "Epoch 186/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0224 - acc: 0.9901 - val_loss: 2.2527 - val_acc: 0.6716\n",
            "Epoch 187/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0128 - acc: 0.9975 - val_loss: 1.7074 - val_acc: 0.7333\n",
            "Epoch 188/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0348 - acc: 0.9889 - val_loss: 2.3090 - val_acc: 0.6815\n",
            "Epoch 189/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0152 - acc: 0.9938 - val_loss: 2.5552 - val_acc: 0.6519\n",
            "Epoch 190/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0165 - acc: 0.9975 - val_loss: 2.2374 - val_acc: 0.6667\n",
            "Epoch 191/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0075 - acc: 0.9969 - val_loss: 2.1174 - val_acc: 0.6840\n",
            "Epoch 192/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0119 - acc: 0.9944 - val_loss: 1.9388 - val_acc: 0.7210\n",
            "Epoch 193/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0104 - acc: 0.9957 - val_loss: 2.1121 - val_acc: 0.6765\n",
            "Epoch 194/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0056 - acc: 0.9981 - val_loss: 2.2413 - val_acc: 0.6691\n",
            "Epoch 195/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0157 - acc: 0.9957 - val_loss: 1.9830 - val_acc: 0.7037\n",
            "Epoch 196/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0051 - acc: 0.9981 - val_loss: 2.0773 - val_acc: 0.7160\n",
            "Epoch 197/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0060 - acc: 0.9988 - val_loss: 1.7058 - val_acc: 0.7506\n",
            "Epoch 198/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0082 - acc: 0.9994 - val_loss: 1.8139 - val_acc: 0.7309\n",
            "Epoch 199/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0043 - acc: 0.9981 - val_loss: 1.9270 - val_acc: 0.7284\n",
            "Epoch 200/200\n",
            "1618/1618 [==============================] - 3s 2ms/step - loss: 0.0306 - acc: 0.9944 - val_loss: 1.7103 - val_acc: 0.7358\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "novLPK2nQgij",
        "colab_type": "code",
        "outputId": "03a28727-9fc1-4ed0-c119-f69b27595e3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzsnXd4Y9WZ/z/Hkm259ykeT2UK02Bg\nhqEmwAIJvYQUYElCGklIL2xINv23JdlNW5KQtksKBAghCYFQh4Teh6HNML3aM55xb5JtydL5/XHu\n0b2SZVu2pbEZv5/n8SNZurr36Ore8z1vOe9RWmsEQRAEASBnohsgCIIgTB5EFARBEIQ4IgqCIAhC\nHBEFQRAEIY6IgiAIghBHREEQBEGII6IgTCmUUr9RSv1bmtvuUUqdne02CcJkQkRBEARBiCOiIAhv\nQpRS/olug3BkIqIgTDoct831SqnXlFJBpdT/KaWmK6UeUEp1K6UeUUpVeLa/WCm1SSnVoZR6TCm1\n1PPecUqpDc7n/gAEko51oVLqFeezzyiljkmzjRcopV5WSnUppeqVUt9Mev80Z38dzvvXOK8XKKW+\nr5Taq5TqVEo95bx2hlKqIcV5ONt5/k2l1F1KqVuVUl3ANUqptUqpZ51jNCqlfqKUyvN8frlSap1S\nqk0pdUgp9RWl1AylVEgpVeXZ7nilVLNSKjed7y4c2YgoCJOVy4FzgMXARcADwFeAGsx1+2kApdRi\n4Hbgs8579wP3KqXynA7ybuAWoBL4o7NfnM8eB9wMfBSoAn4B3KOUyk+jfUHgfUA5cAHwcaXUpc5+\n5zrt/bHTplXAK87nvgesBk5x2vQvQCzNc3IJcJdzzN8DUeBzQDVwMnAWcJ3ThhLgEeBBoBZYCPxd\na30QeAx4t2e/7wXu0FpH0myHcAQjoiBMVn6stT6ktd4PPAk8r7V+WWvdB/wFOM7Z7j3AfVrrdU6n\n9j2gANPpngTkAj/SWke01ncBL3qOcS3wC63181rrqNb6t0C/87lh0Vo/prV+XWsd01q/hhGm0523\nrwIe0Vrf7hy3VWv9ilIqB/gg8Bmt9X7nmM9orfvTPCfPaq3vdo7Zq7V+SWv9nNZ6QGu9ByNqtg0X\nAge11t/XWvdprbu11s877/0WuBpAKeUDrsQIpyCIKAiTlkOe570p/i92ntcCe+0bWusYUA/Mct7b\nrxOrPu71PJ8LfMFxv3QopTqA2c7nhkUpdaJS6lHH7dIJfAwzYsfZx84UH6vGuK9SvZcO9UltWKyU\n+ptS6qDjUvqPNNoA8FdgmVJqPsYa69RavzDGNglHGCIKwpudA5jOHQCllMJ0iPuBRmCW85pljud5\nPfDvWutyz1+h1vr2NI57G3APMFtrXQb8HLDHqQeOSvGZFqBviPeCQKHne/gwricvySWNfwZsARZp\nrUsx7jVvGxakarhjbd2JsRbei1gJggcRBeHNzp3ABUqps5xA6RcwLqBngGeBAeDTSqlcpdQ7gLWe\nz/4K+Jgz6ldKqSIngFySxnFLgDatdZ9Sai3GZWT5PXC2UurdSim/UqpKKbXKsWJuBn6glKpVSvmU\nUic7MYxtQMA5fi7wVWCk2EYJ0AX0KKWOBj7uee9vwEyl1GeVUvlKqRKl1Ime938HXANcjIiC4EFE\nQXhTo7Xeihnx/hgzEr8IuEhrHdZah4F3YDq/Nkz84c+ez64HPgL8BGgHdjjbpsN1wLeVUt3A1zHi\nZPe7DzgfI1BtmCDzsc7bXwRex8Q22oDvAjla605nn/+LsXKCQEI2Ugq+iBGjbozA/cHThm6Ma+gi\n4CCwHTjT8/7TmAD3Bq2116UmTHGULLIjCFMTpdQ/gNu01v870W0RJg8iCoIwBVFKnQCsw8REuie6\nPcLkQdxHgjDFUEr9FjOH4bMiCEIyYikIgiAIccRSEARBEOK86YpqVVdX63nz5k10MwRBEN5UvPTS\nSy1a6+S5L4N404nCvHnzWL9+/UQ3QxAE4U2FUiqt1GNxHwmCIAhxRBQEQRCEOCIKgiAIQpw3XUwh\nFZFIhIaGBvr6+ia6KVklEAhQV1dHbq6shSIIQnY4IkShoaGBkpIS5s2bR2JBzCMHrTWtra00NDQw\nf/78iW6OIAhHKFlzHymlblZKNSmlNg7xvlJK3aiU2qHMsovHj/VYfX19VFVVHbGCAKCUoqqq6oi3\nhgRBmFiyGVP4DXDuMO+fByxy/q7F1IYfM0eyIFimwncUBGFiyZr7SGv9hFJq3jCbXAL8zlkV6zml\nVLlSaqbWujFbbRLefGitaWjvJRrTFOT5qCjMI88/efIjGtpDNHb2UVGYx4LqInJyEoU7FtN09Ebo\n6jXLH08vDVCQ5xvXMevbQmw60MWq2eXMKAsMej8UHiBHKQK5Prr6Iry4u43tTT0smVHC0TNKqCjM\nw5ej6I1EaQ+GaQ2GaQ+GaQ9FiMWcsjcKFk4rZuWsMnJ97vnesK+dzlCEmeUBuvsGCA/EGIhpOkJh\n+iMxfDmKOVWF1FUUUFaQS3ffAA3tvexrC1KQ62dGWYDZFQVsb+qhNxLltIXV8f1vPdhNS08/vhxF\nZVEeR9UU43POZ3ggRmuwn2klAXw5Cq01Sin6IlEe2XyIXF8ORXl+7LipLxKlIxShMM/H4hklHFVT\nHP89thzsprwwl5llAVqDYcoKcsn15VDfFiKQ66OmZPAyFlprmrr7Cfh91LeHeHpHC5VFeVQU5tHd\nH+GEeZXUVRTG29rdF6GqOJ9X6zt4akcLpQE/+X73d186s5SVdWUJxwj2D9DdN4Dfp3hiWzN7WoIo\npairKCCQ62Nncw9nHT190OcyzUTGFGaRuLxgg/PaIFFQSl2LsSaYM2dO8tsTTkdHB7fddhvXXXfd\nqD53/vnnc9ttt1FeXp6RdsRimn7ngnxpbzubG7s42NXH0pmlnLawmtryAn766A7agmHef8o85lUV\nkefPid94Y2Vva5CndrTQ0h3mmNllnLlk2qBtBqIxbn+xnjMW1zC7snDQ+32RKDuaelheW8qGfe38\n4cV6CnJ9PLOzle1NPfHtlIK6igJOmFfJO4+vo6Ykn7qKwmE7WlvfayhLqz0YpjjgJ9eXw0ObDgLw\ntmXTE7bv7otw2/P7uHN9Pb4chS8nh+buflp63OWVj6op4pxlM9jc2EVhno88fw6Pbmmiq28gvo0/\nR7F8VhlLphdz6apZnLKwmmS01uxs7uH53W0U5/u5+FizOuiGfe3c8uxe7n2tkajTeV+6qpZP/tMi\nbnpsB1pDYZ6PP21oIEcpjp5Rwuv7O4lEx17frKIwl8+ctYi3r5jBX17ez389uHXM+0pFTUk+a+dX\n0tYT5tldrQnvnTi/kv98x0q+/tdNPLurlWhMk+fPIc+XQ18kyokLKtndHORA5/AuVaXgvSfNRQEP\nbTrEwS6zfSA3h75IjOriPI6eUcpTO1rw5Sjetmw6X71wGbPKC+iLRLn56d3ctb6BXS3BYY+xorYM\npYy49Q/EmFaST1P30MtvHz2jhGPqygjk+tjXFuKZna2EB2LDHqOqOD/ropDVgniOpfA3rfWKFO/9\nDfiO1vop5/+/A19yFj4ZkjVr1ujkGc2bN29m6dKlmWr2qNmzZw8XXnghGzcmhk8GBgbw+zOru6m+\nazSmeXRLE9+4ZxP7O3rjr+coKC/Moy0YBtyboCDXR28kCkBBro/Vcyt415o6TpxfxePbmtjc2M2h\nrj6UguPnVLBoegm3PLuXQ119RKIxOkIR/D5Fcb6f9lCYQ12JF/6nz1rEZ89alDBq/ta9m/j103so\nK8jlR+9ZxZlHT+Olve283tDBPx09nc/d+Qov7W1n8fRidjT1UJRvzttRNcVcdtwsSgJ+QuEoTd39\n7Gzu4YmtzXT3m862rqKAv1x3KjUl+WhthDGQ66MvEuXnj+/knlcP0B4M8/EzjqKsIJfdLSFKC/zs\nbg7y4p429rSGmF9dxNuWT+cXj+8C4Lg55VxxwmyK83N5Ylsz9752gFA4yonzK6kozGMgFqOyKI/l\ntWXMqy7iQEcvv31mD1sPdbNkeokjzgOcvriGFbNKKQ2YjLEdzT28vK+dLQe76e4b4IfvWRXv9AH+\nuL6emx7byW5PB3TygiragmG2HuqmON/PFSfM5m3LZ/Do1iZ+/vhOtDa/Y1G+j/ZQhEtW1VKQ62Pj\n/k5OWlDFGUumsWRGCZsbu9jTGqQjFEFrTb7fR2VRnhnxFuVRUZiL3xm1D0RjbDrQxa3P7eWZnW5n\nffGxtbz35Lkc7OyjrCCXQK4PXw6UFeRRkOcjPBBjT0uQA529dPUOUBLwM6M0wLzqQvoiMQ509LKv\nLcS8qiI08OcNDbzR2MVAVPP+U+ayanYFkWiMzY1d/OcDW4jGNIV5Pt538jxmlQdoaO8lHI2hUDyx\nvZnSgJ/PnL2YqqI8QuFovJ15/hzKC3IJhaPc8eI+bnluL/n+HE5fXMPZS6fT3TdAfXuIWeUFvLC7\njY37O7l8dR3haIxbn92LL0fxlsU1rN/TxqGufk45qoqzlk4HoKwgl9MX19DdF6G7b4BcXw4PbGzk\nlfoOwFhY00sDbNzfyYpZZVxxwmzC0VhcnKNRzePbmrj/9YNsb+ohEjUC8pZFNSyoKaI3HGXt/EqO\nqSsjGtPsawvRG4myoLp4XFamUuolrfWaEbebQFH4BfCYXQ9XKbUVOGMk99FkFIUrrriCv/71ryxZ\nsoTc3FwCgQAVFRVs2bKFbdu2cemll1JfX09fXx+f+cxnuPbaawG3ZEdPTw/nnXcep512Gs888wwz\na2v531vvJD8QoKoon1yfoqUnTGmBn13bt8W/68HOPj5660u81tCB1rBkegmXHjeLQG4Ox84uZ3lt\nKfl+Hw3tIR7d2syr9R1cccJsFk4r5t5XDxAMR2ns6OWJ7S0JnVBRno+Z5QVEojH2toYAqC7O45i6\ncnw5ivKCXKIxTXf/AOUFuSyZUcI5y6ZTU5LP1/+6ibteauCCY2byvXceSzga4+eP7+Rnj+3knavr\n2Li/ky0HTce59ZBbtTnXp/jgafN5ansLy2aW8vWLllESGDr1NhQe4OkdrbQF+/nGPZtYOK2Yojw/\nL+/rIByNsWp2OaHwANsO9XDqwipylOLJ7S0A+HIU0ZimvDCXNXPNzXf7C/to7OzjvBUzeOviGm56\nbAf1bUZgC/N8XLByJlefNJdjZw9t1XkFaSS6+yJ86DfreXFvG3d97GRWz63k9hf28eU/v86q2eW8\nc3Udpy6s5qntzXzngS0snF7CVWtnc+ExtXHBBHhsaxMPbTrIdWcspK6iIO3jp4vWmhd2t7GzOUhR\nvo+Ljqkd5CLLFuveOMTvnt3DVy9YxpIZ6ayQOjQHO/soLfBTmDfyIG1va5Ab/vQ69e0hjp5Rwkfe\nsoATF1SN6/iTgTeDKFwAfBKzbOGJwI1a67XJ2yUzkih8695NvHGga9xt97KstpRvXLR8yPe9lsJj\njz3GBRdcwMaNG+Opo21tbVRWVtLb28sJJ5zA448/TlVVVYIoLFy4kPXr17No6Qouf+e7eOs553Lh\nO95DeWEe5QW57GkNUpzvp7+lnqVLl9LQHuKqXz1Pa08/15xqXEGXrJo1Jn97LKZZt/kQu1uCnLGk\nhiXTS+Kuk82NXexs7uHspdPT6my01vzyiV1858Et+HMUMW0smUtX1fK9dx3LQExzxwv7+ONLDZw4\nv4pLj6vlj+sbOGfZdN66eMRaXSm5//VGPnHbBuZUFvK2ZdMpyPPzwOuNhMJR/uMdKznd2e/rDZ0U\n5OVwVE0xvZEoAb8v3sF1hMI8ub2F81bMwO/LQWvNG41dRKKa5bWlCX71TBHsH+DM7z3G7MpCPvKW\nBXz89y9x+uIafvHe1Qn+Z+s/F4TxkK4oZC2moJS6HTgDqFZKNQDfAHIBtNY/B+7HCMIOIAR8IFtt\nOdysXbs2YS7BjTfeyF/+8hcA6uvr2b59O1VViSOPefPmUzF7Ebtagqw49jgiHU3UlOTT3N1PKDyA\nQtHTP0DUCQ6+7+YXaA+FufXDJ3LcnIpxtTcnR/H25TNSvrd0ZilLZ5amvS+lFB89/ShWzCrjie3N\n+HMU56+cyfJa4wf1++CaU+dzzanu+TmmbnwxlfNXzuT5r5xFdVF+vJP//DmLB23n9cUmjxjLC/O4\nyOPGUUrF25wtivL9fO6cxXz5z6/zSv0Gjq0r5+dXJwqCbYsgHC6ymX105Qjva+ATmT7ucCP6w0VR\nUVH8+WOPPcbD69bx5wf+gS8vwPsvv2DQXINoTKP8uQT7o1QW5VFTWkBvKEhNcT6tPWHCAzFmlRfQ\n3NPPgWCYT/74KZp7+rn1Q+MXhGxx6sJqTk0RRM0W00oGZ+G8GXjX6jp+8/Qeuvoi/PJ9qzPq+hGE\nsXBEzGieaEpKSujuNv7xnr4IveEoA9EYfl8O7e0d5BWW0Kv97Nr0Bs89/xzJLru2njBaa+ZWFVKU\n749nA/l9OUwrzaerd4CKojwCuT6a63NYUFPEv126grXzKw/7dxUyi9+Xw50fOxmliAejBWEiEVHI\nAFVVVZx66qkcvWw5/tx8Kmtq4hktJ51+Ft+/8SdcftZJHLVwEccct4ZOJ2cdoD8SpTVkcrO9AUTL\ntJIA05wYW1G+n+rifG750KrD9dWEw0BZgYiBMHl4063RPBmzjwaiMerbe+nui1BekEdpgZ/6thBF\n+f54oHXxdDN5Zk9riN5wlKNnlqC1ZkdTkGhMs3BacVpB4on+roIgvDmZ8EDzVCGmtenoI1Fqywuo\nKspDKZN109Bu0jlnlgXiwcLq4jx2twTpDEXo7I0QHogxv7poUs3SFQRh6iKiMEaisRjB/iidvRFC\n4QHmVBZSXpgXf7+yKI9oTNMWDCe8Xpxvprvv7+glpjW15QUUB+RnEARhciC90Rhp7OijLWRmCteU\n5Cd0/JaakvxBdVSUMnVdGjt7qSjMo6po8OcEQRAmChGFMRDTms7eCGUFpqhWnn90aYRVxXnk+hSl\ngVzJQRcEYVIhojAGuvsGiGpNZVHeqAUBIEeplJaFIAjCRCPRzTHQGYrgz8lJmUIqCILwZkZEYZRE\nY5quvghlBX5yHNdPR0cHN91005j296Mf/YhQKJTJJgqCIIwZEYVR0t0XIaY1ZR73j4iCIAhHCuL/\nGCWdvRFnlSc3lnDDDTewc+dOVq1axTnnnMO0adO488476e/v57LLLuNb3/oWwWCQd7/73TQ0NBCN\nRvna177GoUOHOHDgAGeeeSbV1dU8+uijE/jNBEEQjkRReOAGOPh6Zvc5YyWc9x2isRhdfQPxCWqW\n73znO2zcuJFXXnmFhx9+mLvuuosXXngBrTUXX3wxTzzxBM3NzdTW1nLfffcB0NnZSVlZGT/4wQ94\n9NFHqa4+fMXjBEEQhkLcR6Ogq3cArfWwtWoefvhhHn74YY477jiOP/54tmzZwvbt21m5ciXr1q3j\nS1/6Ek8++SRlZdktyywIgjAWjjxL4bzvZGW3fZEojV195PtzKBxhPeAvf/nLfPSjHx303oYNG7j/\n/vv56le/yllnncXXv/71rLRVEARhrIilkAYD0Ri7moMoYG5V0aAJZ97S2W9/+9u5+eab6enpgegA\n+7e/TpMTOygsLOTqq6/m+uuvZ8OGDYM+KwiCMNEceZZCFujpH2AgFuOomuKUi6DY0tkrVqzgvPPO\n46qrruLkk0+GWJTigJ9bb7uDHXvquf7668nJySE3N5ef/exnAFx77bWce+651NbWSqBZEIQJR0pn\np8GBjl7agmGW1ZbG5yakRfdB6G6EqkWQX5yRtkjpbEEQxkK6pbPFfZQGwf4BCvJ8oxMEgNhA4qMg\nCMIkR0RhBKIxTV8kRlHeGDxtsajzGBl+O+HwEOl1fxNBEFJyxIhCttxgveEBNJrC/DEsqG4thGhm\nLIU3m6tv0nHj8fDCrya6FYIwqTkiRCEQCNDa2pqVTjMYNiPL4dJQh0RnzlLQWtPa2kogEBj3vqYk\nA2HoPgAtWye6JYIwqTkiso/q6upoaGigubk54/tu6eknGtNs7x5DZ9zdCNEI5PZAUXDcbQkEAtTV\n1Y17P1OSiHP+Q60T2w5BmOQcEaKQm5vL/Pnzs7Lvtf/+CG9ZVMP33z2GjJ/vXQo9B6HuBPjwI5lv\nnJA+YafoYKhtYtshCJOcI8J9lC2au/tp6u5nWW3p8Bvuex4e/trg13vbzWPPocw3ThgdESsKYikI\nwnCIKAzD5sYuAJbOLBl+wy33wjM3Qn+P+1qkF6L9oHKgpxkkSDyxhMV9JAjpIKIwDG84orBs5giW\nQqTXPHY3uq/1dpjHinkw0Av9UspiQvFaCiLQgjAkIgrD8MaBLmaVF4y8nrL1V3cdcF+zrqPqJeYx\nmPkguDAK7G8UG4D+roltiyBMYkQUhuGNxi6WjmQlgJvZ4hWFPsdSqFlsHiWuMLFEPNlfoVY4tAmC\n4koShGREFIagLxJlV3PPyEFm8LiPhrEUepoy20BhdIQ9S54GW+E3F8CT35u49gjCJEVEYQg2Hegi\npmHZSEFmGMJ9ZC0FEYVJgddSaN5sRNv7ewmCAGRZFJRS5yqltiqldiilbkjx/hyl1KNKqZeVUq8p\npc7PZntGw5Pbm1EK1s6vGnnjVO4jaylULjAZSEERhQnFaynUv2Aee2XOgiAkkzVRUEr5gJ8C5wHL\ngCuVUsuSNvsqcKfW+jjgCuCmbLVntDy2tZlj68qpLBohyAyu+yg5pqByIFAORTUSU5hoIh5RaHBK\nr8tENkEYRDYthbXADq31Lq11GLgDuCRpGw1Yp30ZMCns+bZgmFcbOjhjSU16Hxgq+yhQBjk5UFDh\nupOEiSEcBH8AfHnQvMW8JqIgCIPIpijMAuo9/zc4r3n5JnC1UqoBuB/4VKodKaWuVUqtV0qtz0Z9\no2Se3N6M1nDGkmnpfcC6j4JNpvAaGBEoqDDP80tknsJEEwlBbiEUVmHGIsicBUFIwUQHmq8EfqO1\nrgPOB25RSg1qk9b6l1rrNVrrNTU1aY7ex8HjW5upKMxl5ayy9D4Q6TVWAZhaR+BYCuXmeX6p5MZP\nNOEQ5BU5ouAQ7U90KwmCkFVR2A/M9vxf57zm5UPAnQBa62eBAFCdxTaNiNaap3a0cNqiGnw5aay0\nFouZjqVqkfnfupD6kiyFPhGFjBIdgOd+BgP96W0fCTqWQqX5P8/JKpOyF4KQQDZF4UVgkVJqvlIq\nDxNIvidpm33AWQBKqaUYUZjQqb87m4M0dfdzylFpZB2BKWEBUG1FYb+pdRRqhQLHUgiUivso09Q/\nBw/eALseT2/7cAjyCl1LYdbx5lFEQRASyFrpbK31gFLqk8BDgA+4WWu9SSn1bWC91voe4AvAr5RS\nn8M4eq/RE7y82LO7TCdx8oI0RcFmHlUdZR7vvg4G+szzRW83j+I+yjw2myucpthGQpDrcR/VnQC7\nH5dgsyAkkdX1FLTW92MCyN7Xvu55/gZwajbbMFqe3dlCbVmAuVWF6X3AVt8smWlEQCmYfzr482HJ\neea9/FLTKUUHwHdELGEx8fQ4BmU4zZhAOAjF0xJFAUQUBCEJ6aE8xGKa53a1ccaSGpRKI54AbqAy\ntxD++c7U2wScrNv+LtenPV5ad8Ibd8NpnzdCNNWwkwHDaa5oZ7OPFpwBja9B7XHm9UxNYOvvNpZI\nzkTnbgjC+JAr2MO2pm7agmFOOWoUse6wRxSGIt8JambShbT5Hvj7t6Fjb+b2OZHEou4s8HSwZUMi\nQ4jCvufhmZ+4/9vso7mnwFV3QFE1oDITU4j0wg+Xw6u3jX9fgjDBiCh4eL2hE4Dj55Sn/yFrKeQN\nJwrWUshgsDkaMY8HN2Zun5kgHIL7vjh6t8yrd8CPjkl/5B8cwX30wi/g4a+67bDZR5Ycn0kEyIT7\nqLMB+jqhafP49zVaYlGpqyVkFBEFD9ubesjz5zC3qij9D8XdR8N8xrqPMpmWGnUmyR18PXP7zAQN\nL8KLv4JtD47ucx17jSXVsS+97XtGcB81bwU07H7C2S40WLgLqzJjKXQ6czQzWcqkfY9Jdx6JF/8P\n/mdV+mIqCCMgouBh26FujqopTm9+gsXejMNaCtZ9lElLwRGFQ0NYCi/8Cjb+KXPHSxfro296Y3Sf\ns+exsyG97a2lkMp9FB2Alu3m+e4nzGg62j9YuAsqMyQKTpszJQrb18H/HAtb7x95212PmXPQUZ/4\nutbQsiMz7RGmFCIKHrYf6mHRtOKRN2zZYToecFNScwuG3j7fme2cyZhC3H00hKXwzI/hpd9k7njp\nYjvZQ6MVBWd963QsBa09lkIK91HHXiMCOX6TdjqUcBdWjS/Q3LzNXAedzpzM7gyIQjgIf/u8ed74\n6vDbag0NTsXXriQx3fgn+Mlqk5AgCKNARMGhp3+A/R29LJ4+gij0NMNNJ5rMH0jPfWQthb7O8TfU\nYkWhY+/g/WoN3QfdtM3DScgJFo/Wv97viEJn/fDbgRHXqDOTOVWZCnvsZZdA6w5odayG5GSAwsrB\nMYXOhvTOW0c93HQSvPL7zFoKT/8PdO4zM65btg6/bftu12JKtrA2/tk8TiZRCLUZt9hYaN8Df3iv\ne50IWUNEwWFHk7nYFk0fYVGdrgazzm880JmG+yiQjUBz2H1+aJN5bN5mVhXrbTed5kSs4WAthe4D\no8smsucx2Q2SCm+nHU7RSdgqqGuvNY9bnfhGXpJwJ4tCNAI3nwv3f9F9LdQGt70HuhoTP7vvOdBR\n2L/eFbK+jvTLbgxFw3qYuQrmv8WJiwxD/Yvuc68ohEOw8x/O62mcz8PFuq/Dr/4JIn2j/+yW+0zG\n3cHX3Nc6G9K7XoRRIaLgsO2Q6bAXjyQKwRbzaEeo1n3kH8Z95A9ATm7m3Uf+gHn+6u1w72fhpyfA\nuq+59ZdCba6bayzcfZ3buaSL1x0zGmshPApLwYpdjj+1+6h5K5TNhrq1kFcMe54yrw+yFKpMmRK7\nj013m+N3H3S3aXjRBM33Pp342frnzePBjYkd8ngzgfq7Tc2s6sVmlD/c79fwgvl+JbWJbdj5D7f8\nSldyuTGHrkZo2zW+to6Wlu1m0LDlb6P/rI1ReUXg7uvgN+e7lYm9DPTDbVfAgVdG3nfja/D7d0/u\nYH04CD9cAa/flfVDiSg4bD+QyOUMAAAgAElEQVTUTZ4/hzmVI8xkjgc4nZsuEjSCMNykJaUyXz47\nGobSWiibAxt+Bxt+a6qyNm+Bbjuq1RBqGdv+B8LGNbJ93eg+F2qF4hnmubVg0iEuCsMEmjsb4NcX\nuL72stmp3UfNm80yqDk5MH05HNhgXk+25kqdSu4HXzMut2d/bP7v86x9YWMcyUt3Wl9+02bT8VYv\nNv+P5EJq2gK/Pt+NQyTT322ulZolEIsYF9FQ1L8As1ZD+ZzE87blb+ZaSBYLLw/8C/zxA8O3NdPY\nc7nhd6P/rI1RdXpiTs1bzT43/Hbw9gc3wrYH4I2/jrzvbQ/B9odM0H6i0drU80rOPDu0yQxYkq3d\nLCCi4LC9qSd15lHbrsTOLT5pyhGFVKmOqQiUZj4l1ZcPH38KPvkSfGErLL8M2nZ7RIGxj1xtJz3a\nxYFCrTBjhQmup8pAev0u+MvHUxzPGaV1N7rxkmT2vwR7n3InpVXMc9u58x/mc7GoGZHWHG1en77C\nrUWVHPc5+gLTeT77U9j+sBGb/NLE72wtF+85DQdNp1M224zIB/pM5wypRWHfc3DzeY5b5+/G6rj/\n+tTfsb/btKHaWdt7KBdSNGKuy1nHQ1md2/kHW2DzvbDkfKiYO7QodOxLP9MrEwz0m3MYKDfB/9FY\nKbGY6xK0lkI45Japf+K/B8fVmpx7dqjsPC+2LaMdAGWDxlfgdxcbkfJiLZ6Zx2a9CSIKmHLZG/d3\nsXRGCtfRQ/9qRnb2ohtkKfQOH2S25Jdk3n3kyzXrOFQvNHV9KuYZ9423IwmOMdhs29o3hCg0bYFf\nnTU4bhBqM26Z6ctSu4+23Acb7xq8uE04aNxBOja0y8P6/7saAAXls03n0LIdbrnMdIZdB0wnXbXQ\nbDtjpfv5ZPHOK4I1HzAj6z9fCzVL4Zj3JHYwHSlE4cDLJp6w+v3ua8OJwlM/hH3PGIukZZt5bet9\npr3JWEvBVt0dKtjc3WjaUDHPiELXftN5PvVDYz2d9tlEsUimp8kIeCya+v1M09kAaDjlU2b1u/u+\nmP6xO/a4FqEVaRuwXvtRc43fdAo8fSOsv9lcE/baS2ceT5sTjN/xSOJ1ORCGp35krvN130ivrSPR\n2w63vAOeHWLlYWuRJk9KbXzVLOtbMjMz7RgGEQVgb2uIlp5+Vs+rGPxm137TMT77U/N/PKbgcR8N\nl45qyS/LvPvIl5v4WuV88+j1f49ZFJy2DmUp7H/JBFkbXkp83YpC+ZzUnXtnvWl7stunv8ftyIcK\nHnrjFYVVbqFBGwPo2u9aRvbm8YpCKvFe+1FQPiNGV/weSqY7o/9+t72QGGiud1xHx73XCBk4tZTU\n4LTUzv3GCgFjObVsd1w+c+HlWxO31dpUfc0vMZZl6SyTPJAK634qqzN/0bARnRd+BcdcYdxPZXWm\nk0nufGMxJy6jxzaju78bNv9tdKvW2XIsc06C875rLKbHv5veZ20HXzrLvTasW+2Y98AHHjTnbN3X\n4G+fM+fVWvfdjSb5YjjadpnYTGd94oBq12PwyDfMvjLhyw+1wW8vMt/dXhPJ2PvVWkaWxleNlXAY\n6pyJKAAv7jE3xgnzUhSrs53Msz81ghC3FJxObULdR3mJr1U4otD4mvt8rO4jKwpDWQrWbeO9eAfC\nplMrqDSjmp7mwR2Hvam9FobWZn/W5TNUsLm33QTslc9YRnlF5newcZOeQ+5IvdhZSnXaUsC5kVL9\nTqUz4V2/gffebcqf29XyrBjakXa3J6bQtNnEckpmuG6einlGqJIthVduM4Ljyzd+8ZZtMG2ZEfBk\nKysSMtvmO2nR1YsHdw4W266y2abzB7O+hI7CGV9y3qszcYnka6C3zWTQwdgGDa/eAX/4ZxOETxcb\nTyifA6s/ACvfBU/+IL0UUxtPWHSO+d5aGzcpmPM450T4+DPwL7uh8ijT4Ta9YX4jgEPDWAt9XeYc\nHHul+X+Hx4Vk4xer32+s0/Gus/7k9813qVo09DUeFwWPOEX6TJzsMLiOQEQBgJf2tlMa8LOwJmmO\nQixmbqhFbzed1rYHx+k+yuA8hdjA0JaCjhr3g79gcFrqI9+Cv35y5P33jxBTsKLh7bTsSL6w0nTK\nA72JKaMD/Z7lSjsSX9dRjygM4fIItZv9rnynKX1tg27W5O5pcr+vFYW8Ineti6GKFi69EOoc949d\nLa+v04hc90FQOebRClxvm1NQD2OJ+AuMIBRPH9wBv3obzH+ruaH3PmOun+rFxu2X7Ae359TOa6lZ\nYiyLVOUubKdSOssVhX3PmrhSxTznPed1a7E9+GUTy/AKV/L18cKv4DcXpj5PFtshj2b03LHPiHlJ\nrRntHvdeI1i2DMlwNL1hLKuapeaaCrUaSyG/zP29cnLMdbfoHDPCDzab6wSGrw9m4wnz3wrTlhv3\npqXrgLEEF5zhtGMU2XTRAfi/txnhA/Nbv/Rb8/ssfrsrbslYT0TLNtfCa3rD3O8iCoeP9XvbWT23\ngpzkIHNvu+msFpxhRuXNW11RGBit+ygDq69tuAV+cbp5nspSyC+BQqezKpkJxTWDJ2K9fpfr/hiO\nVDGFPU/Bjccb/38qSyHkEYUip1P2dpLezt47Srb7KqwyHVnLEC6T3jbTCbzjl3DxjW4nH5881uQe\nr8izlvf0FeYxncwNu9Z2X4cTu9Dm89Gw+/1CbW4J9NP/xVgaShkh6vGks/Z1mk7nqH9yYiyOS2NE\nUSh1t4sEh3DDNRirJr/YFQWAkz2Cb1+3ArJ9ncm0SRCFpOy0TX+BPU8OzrbyYv35m/6Sfspzxz4o\nm+WuJzLnZOOy2ZFGcPfQJmNdlc9299W+ByrnDXanLDzHiA2Yjr54xvBxBRtPqFwAKy4zwmqt2c79\n5j6y189QpVtSde67HzNpy3//lokdvfQbY0Wf8kljLQ30pbbS7O8R7XfPc+PhCzKDiALtwTA7mnpY\nk9J15Nw8JTOMv7tl22BLIV33kV2neTwLy73ye3OBxGKpRQFca6G01nSM3pFgV6MxiW3bh8N2UJGQ\nmwd+4BVzE3UdcC2J5q3ud7IT1wqrjCBB4oU/kijkFUHtKti/IXWbQm3uyNBuD65rwopCQYVZ5Mhy\n9IUw+6TBllUqvO4j2znMXmserQupt91tR9VRsORc87xkRqII2nhAzdGmU7PUDCUKjhB7LQVIHWzu\n2m9cR7bN+aUw7y3m/FniouCMSjvrzZ83ZuP9faIR99wP9RuAiQ/kFZtr6+kfGsvzv46Cez5l3tc6\nhdtwnxntW/x5ZjGq7Y8Mf0+E2sz3r1uTKHJtu10XqZd5p7rzd6YtM5bccBlI1lKonA8rLjfPNzmz\nwbv2u5ZYfmlqUejcD98/GrYlxQhe/5OxZGaugj9cbSbuzT3NxJ7s75YqdhZsduc8NW8x52brg+Y3\n9p6/LDLlReHxbeamWDM3RZA57oqYbtwxDS+6vtjRuo8CpcbqSKdDTkVvhzvCj/a72UfJ2BulZKYZ\nrXstBZtbPzAKUQDXWohbD11uR97f5WbmWPdRwVCWgucmSBAFJx01v9ikWLbtTD0burc9cZEiKwp2\nv8EmI+T22JZj3gUfSkrxGwq7rnZfp7vfOkcUbLDZKwpeiqcbN5Pt7K0VVbPEFQVfnrm5A2WJgguD\n3UfxtNQUllNng9tJKgXv/h1c8pPEbQJlTgDVCcAP9JmYhb0OUIm/z8HX3WvjwBCioLUZwa58pynF\n8Y9/M3MBCqvgldvNSPf2K002l5dkUQBYdLYZpDz9I+NaScW+58zj3FPczrR9j9lfZQpRyC0w4lhU\nYyy36kXDp7+27Tb3Sl6RsRZmrYbX/2je69pvBldKmdhUKvfR49811uG+Z93XIr3GOlh6EfzzXfC2\nf4Ozv2msW3AtHu+cC0uwBWY7qwI2b4HnbjLpqad99rAtpjWlRaGnf4DvPriFJdNLWJ1KFHq8orDY\nHQkrn2dGczBNS8Gz+tpY2PWoERUwN/dIloJ1H3ktBSsqqYTp4Ovw8u/d/72iYDtoGyjv70p8394s\nCZaC0zF7j98xhCj0ey2F483zAy8PbmNvkqVgxdibo9/d6B57LFhLoc9aCsqMUsHsOxY1nX4qUVh+\nmXFd2Ey15i1m1Fc+1xWFqoVmLQd7HO/1kCwKRdXmOKkshc56446xHHWmG0uwKGWEo2NfYqHBfc+Z\nc1c8PdFSsIHj4hnDW2s2KeDqu0ynd/0OI0qxCPz1E2bSmJ1FDu4chfI5iftaeI55fOSbcO+nU2cJ\n7X3aBOlrjzfnIq8Ydj9pjpXKUgA4/7/hitvM9y+ocMVX68FlSFp3muC0ZcU7zb3Q6ljE9hxPW2bc\nWF6rpnWnm0FmhSfYagLK4W5Yebm5B0/5FJz2OTe2ZcUtVews1GK+V2kdvHgzPPw1Y+me+tnU3zUL\nTGlR+OG6bTR29vEf71iJ35fiVMQzWWrcGatgLhSv+yidmELcVz3GYPP2R9znA2FjKeSksBSqnPz2\n8jlmxBxscQOV9qaP9A422Z//hbmh7WjYGyDuTbIU+h1LwXZCNlMiLgqVbmyjp9l0pJE+05GVzDRi\nltJ9VOwuk5ncKWntjNC9lkJh4nHRZv7EuETB+Z16O8xNWzLDvYm7G53fTye2w1K7yhThs5lqzVvM\nSDXHB0VVxhVhg+mprodkUVDKWAvJlkJ/t/mcN5YwFDVLjNvDu0Jf6w6TeltUkxhTqH/BBIKXnGss\nhVRuHevnLp9r0ksXnWNcddOOhtknuutodB9wv0+rU8I7WRTKZ8PVf4Lz/tv8n2ogsPcZI8q5AXM+\nKua7cYiqowZvD2ZgZF1+AY/l9+L/GldP606T4nnLO4zV5N3PAidmt+1BM/iywfppy8xAwVrF9S/C\nHVeZa7n2OGPdRnrhZ6eYyXSzT4R5b03dvkCZsbKS3UexmPk9imrMTPyuBjj2Crjs54d1yd0pKwp9\nkSi/f34vlx9fl9pKAGMp+APODNNF7uvlc80FEIsZczsd95ENfI6lkqbWZmKNzYmPWwopRGH5pfDe\nv5ibtHiasS5628wI6cDLzgSx6OBZwz1O3vqmv5j/vSNY6z6yHVhflxndV8w3naN1k4TaTcfuzzcB\nxYJKYyk89BVzs7TuMB1sQUViADteVLDYuG+qFg4Whf5u47rzuo+82UQ+J4YQ7jYj4LHizzP77esw\nHWnZbPNaUY0ZOVoxS2UpAJz5r2Zk+syNRiytCABcdadxJUBiQNv7HcG1KsF06slpqfE5CrNH/j7T\nV5pMHZu3b89Z8XRjiXgthfoXTGdae7wbJE+mY495rEjh3z7emcy36O3m0a5p8cY9gHKzeLwsPBtW\nXWne3++Z87L+ZmO5Nr5qXEeWy34GF/0PXP5/MOeUQbsbRIHH8mvZZu6F295tMqwObTSj+NP/xd2+\neok5R3ZiYWmteZzhBJs33ws7/g43v838Xlf83ghA226T5dRzEC7+MXzwITeonoxyJl4mp6X2dZh7\ns6gaLvgefORRuPQmd5BwmBii1Uc+T+9ooS8S45JVtUNv1NNkRttKuSNwMCOe+hfSW4rTYi+u5Gqb\n6dDdaC62OScb32U0PLT7yJdrsl3AI0RNTnG1sLuPgV7T2Vmsm2fjXXDydeaCz8k1Znpvkij0d5mO\nvHSm6cBt5xFqTRxBF08zx+7Ya0ZSbTtNMK+/K3VMwcYIao83GTBe4vEKb6DZk0JcvdjNRx+PpQBm\ndNnXYb7XfGe0VzLDxAtGEoWaJbD0Ylj/a/M9a65x37MdCwxhKThC7P1eNUtMbZ+uRnN+AqWeOQpp\nWAr2mNsedCcVHnjZnCN/wM2+6TpgfNwnfcydnb3nqcGjca+lkMyxV5jzVDrL+MFbtptR9Ma7YN5p\n5npJha31ZOMYvR1w3xdM/APMNRv/PisTJySOhPc8h9pMh9+603yv9/118Dn0+U1w2MYIrPto9knm\nvnr4q2YfNUfDBx80+2/daaxdOyFtwZkjj+zLZg+2FKzVVlRjfqdky+owMWUthUc2H6I438+JC1K4\nASzBJreDyS92Cqgp8xjtd0d53pHdUNgZtt1JqX7hEDzwpeHdSrbTtX7pgT6TCphKFLxYl9f+9cbS\nUDnG3IfBcYWeJiMC+18yx+vvdm+IoQLNeSXOiMfppHrbEkfydnTtnYhjLYWh3Edggs3djYmzg0Oe\nILbFK8bTPCPy5EDzaAmUmXZ37Xc7xZJa81oohTglc8qn3HPltRSSjwGD3Uf+QKJY22Dzz04xf7GY\nu6BOWqLgdKBNb5hOpnKB+b94RqL7aIfjnlxwhpst9cCXBlfJbd9rXIP5xQwixwcLzzLnLMdvYiGN\nrxoL0c4ZGIra4821p7WJI+gYrL4GllyQKAqjxZtN1ttuvtu1j8GH1g19/mYdDziuM+s+yskx1knx\nDNPG99zq/ob2nG76s7k+0/ldymcPDjRbq83OgZkgpqQoxGKaRzY3cfqSGvL9vqE37GlKHHVWL3LK\nKzjmnO207MUxHPnFRjy8ZZnBdNjP/9z4TociLgpLzeNAeGj3kZfpy01HsOU+44etO8F0bpAoCnYl\ns+WXmv+3PuiIguOeiFsKNqbQ7dTocYTS1t3pOmBGipbiaab0QjQMJ3zEvFa10NyoqUTBdjQ2VpEq\nW2ko95FN34TxuY/AuBys+8qW3rA3cap2JFO3xowswf3NkhlKFJJdBdOXOYkNveZ8tO4wI9Mcv1uN\ndjhKZ7kdY/lcN6haPM10PpGQsdS2rzPXxrRlZrT8vnvMd7/9qkQ3Uvue1K4jL75c41ps2WYyeXJy\njfU0HLOON51iZ4OpEuovMLGGK29LzxIfCq/7yA5aalcN//vNcpIdcnIT57sUVsJH/g4feyLRgrLJ\nHa07zL7T8f+XzTa/vff3t6JQKKJw2HltfyfN3f2cs3SEziNZFFZfYxZusYFlG3QKpGEpgOkwkycF\n2U5muFrubbvNBWpHJMNlH3lRymQu7PyHmWOw8By37V5R6G03bqLa441wdew1MQOb7ZFsKfR3OpZC\nsbm4o2FzQSfnjhdNc1N4T/gQfPxZU6umoCJxRnM4aKwYm19uS1p37Tf+879+wp3AlWqeAph2WEuj\n2HMjjwXrPgJP4H6uuYGt+2Q4SwHgnG+b75qcERQ/RpqiUFoL1z1rXBVgAqN7noJZa4b2WXtRyrUW\nEiyF6a5F1dVoZgEvOtvt0Ipr4J/vNOJz3xfdoHPH3qG/k5fqxabcyiu/N4Hr4TphcDvi/S+ZKqpz\nT060mMaKN5ss1JY6QSAZmwFXOnNwSXxbeNJL+Vw33pfuBDP7O/x4Ndz7GSOEcUthnNfvOJmSovDi\nbuMCeMuiYRQ5FjXpYd5R5/LLTF0Z27Ha2av2whuJkpmJ1TbBFYVU6wJY2naZG9p2gpEQoEcWBTDl\noaNhs/2is922e+cqxFNvp5lOqLPBLeEcKDcduNaupRBsMZ29dzZt4ysmPdd7w9jO2ZdvOtfpy8yN\nnuw+6ncExnZI8UlK+01648u3uiUVvDe1L9c9B7beEmTGUrDYm9f6d+3KXyNZh3NONDOvc4awRHML\nTUcykiiAsYJmHGN+i+3rzLmeP0RmSyrsjNzyOW6Moeoo93xtvc8Ivk0RtZTWwllfMwXc3rjbXBft\ne0xbRqJ6kRGQ3vbEWdbDtTG3CB7/LxNYn3962l9vWLzZZL0dI4s5mGu4oNJ1HY2Ez+9eH+mKwtEX\nwDt/bWItr99lymW/fIt5byQBzTJTUhR2twYpL8ylqjh/6I2CLcavmUq145bCKNxHYG6y7oPm7873\nu35OGMFS2GU6J9sB2hF7OiPF2Se5HeaMY1NbCt56QdYdZDuoAmfUPNDnlg+wo/a8EjfuYGvYVCZZ\nCmD8/d62FlQYS8NmQFmrw/u+P2COY+vs2MBz8k1tXUi29hBq/Oa3/T1L61zXhb3pD7xi3h+qs08X\npQbParZCnIqcHOP+23yPuS4XjKLTjFsKc83zT79sMnqs7/rJ7yfW+PFywofNqP/Zm2DL/ea1o0eo\njQRuPGvWapOdMxL+fLj8f931tEfz/YYjN2CupVCrsXDT6XCVgjO/YqzbdLFuuXRFIccHK95hSqRc\nv8PEjhpfNdd3OjPvs8iUzD7a0xJkXtUIaaTJhdW82I7IjvrTCTSDaylsvteMvFa/f2RRsDNI55zs\nulfsZK90LAWfH972/8zznBx3Cn0klaUw3QjXwdeN8OQXu5aCt8KrdYHlF7txB9tpe91H9twlZ4vY\nkXhvh7EmwsFEV5BSrjhZ4dAxc56ThdC6tworzL4Kq9ITy+Gwlp/Xb2yzbboahp40NerjJItC1/Bp\nprPXmtiQv8AIRLosOc+ki851ArbW+pm2zFQrDbWZ6yuVGzTHZz778L86xfyWmPU7RsKW2zj1M+nn\n2B99vpm3sOPvZgCTKQLlHrdfmqPwtR8Z3TFmHmtSXMdybeQWGBH64/sn3HUEU1gUTlxQNfxGtuNN\n1eHbztkGjdO1FEpmGreLTV0Lto7sPgq1ms6icr5bz8fms6cjCgDHXe0+T2UpeIvIldW5gmgthbZd\nns5LuZPF8orNyCa30PiPUYlBSHuBT08WBWe039vuiELP4GJ1ZbOM+8grlqlMfzuSL6iENR9KPdod\nLVa0qjydX2GlcW9Egum5INIhpaUwTE66nZA19+TE2k4jUVjplljwkhswo/OROPYKM+u4fTec9vn0\njjl9OXx+y9BpqEMx/62jc42lQ6DMtTgz9dslc/q/wMmfGPsks6UXmzjRJBCFKec+6otEOdDZx9yq\nETIaok5NmlQdr7UUeg4af3luIL2D2xvErgUbbHYDrkNZCvGCXQvcjsBm64zFzEwpCodMILugwg3y\ngrPYi2MpWJeVd+Wn/GK3lALafNbbWc1YadIzV7wjsQ1xS8FjJQ0KsDqWQvseU0gMUpv+uYUmOydQ\nZko9nPDhdM7C8FiR905YVMp1IU2UKMxabYqsLTk/M8dPl6Jq4wOH9FxHltEKQrYo8FgKhVkSBX/+\n+GIBOTnw/nvgnTdnrk1jbUo2d66UOlcptVUptUMpdcMQ27xbKfWGUmqTUuq2bLYHYF+bGZHPrx7B\nfWT956k63nhM4WD6VgK46aBWcEItqd1HG25x1/C1I5yK+e6s3dFaCqna7g00B5uNq0cpd5IdGCvJ\nxhRs5+XNwc5zOjArJMkFyny5ZgZvsgvOaylAakvBikIkaAqLFVQMYSk41komywDY41QmTdyaaFHI\nL4HPbTQW0eHmn74Kp9/gliF5MxEod6/3dN1HE0Fe0fjSbzNE1kRBKeUDfgqcBywDrlRKLUvaZhHw\nZeBUrfVyIOtVn3a3mM53xJhCdDhRcH64YEv66aiQmMMPQ1sK2x40U/y1diwFxy0zyFIYgyhY11ey\npWA7bm+nby2FSMid5JTwfnHia+mkKsJgUegfwn1kqToKLvxR6qJgeUUmjpBJ5r/VdIDJbgwrCpnK\nDvGKwkC/GSx4A+4pP1M6OE3ycFC9CM788sQce7x4s8my5T46gshmTGEtsENrvQtAKXUHcAngLUr+\nEeCnWut2AK31GNeOTJ89VhRGshSsKKQqOhd3F+nRWQrF000+vo4ZN0ywJXVMobfDjJBDbUYUymYn\numXilsJY3EeOoCXHFKxbKMFSKHHL/NpUzARLwTmHNjiaqpRxKuyNafOyw8HBnaHXjVUxL9GV4+W0\nz429yOBQ5BWZDjCZbFoKqeoeCZnBe49OcLrnm4G0ZF8p9Wel1AVKqdEME2YB3uIeDc5rXhYDi5VS\nTyulnlNKnTvE8a9VSq1XSq1vbh7jQvQOe1qDVBblUVYwQocajykMYynA6ETB5zdpmuVzTO55cAj3\nkX2tY68J7lXOcz5vU1IdSyGVYI2EPx9Qg0UhXs6jxPitwXTUtjO2xcq82TG2I7ej+nQzLwLlxjVj\nq10OKwpq+Bowc092F7nJNtkQBVvWOXmBHSFz2GyyHL+Ibhqk28nfBFwFbFdKfUcptWSkD6SJH1gE\nnAFcCfxKKTVoJpjW+pda6zVa6zU1NeOLzu9uCY4cZAZ3Ju5wMQUY/UU292RTz6WoxvWZQ5IoOPV1\nOva5cxTA+M19+eOLKShl2m99rLGYG1Ow2E4+v8Sd0XvgFUzdJ2+g2enAZq02vlo7KzWdNqx8p6mL\nv+0hU9m0LGm8YP8vqxtdpk02se6xTLmrvGWdk8tmC5nDuo8yHXs6QklLFLTWj2it/xk4HtgDPKKU\nekYp9QGl1FDD1f2AN+m6znnNSwNwj9Y6orXeDWzDiERW0FqzqznI/JHiCeBaCqlG435PttFoLAUw\nk1XO+46ZYOVdezeVpXBoo0n/tKJgjx0eh/sIjChYS6G3zSnX6xEFO0rPLzU+7JKZRkTyS93v68t3\njz9tKXxpd/oxBTCLmaDhjx8wN6s3bRZMh5lbNLp9ZpuZx5qyzZnK/vGWuhBRyB72PEs8IS3Sdgcp\npaqAa4APAy8D/4MRiaFW3n4RWKSUmq+UygOuAO5J2uZujJWAUqoa404aZu288fFqQydN3f3DV0a1\nxAPNKUbjSrkupNGKgsVbCdEf8Kzk1mtmD4OphwKJbhl/3vgsBTCTn6woWJ+294axcQUbSLazUwOl\nrmWUqkrmaKhZbFJWI0ETF0g+j0qZipu2DPhkQClT/2q8390ionB4sBbZZM48mkSkFWhWSv0FWALc\nAlyktbYFfP6glFqf6jNa6wGl1CeBhwAfcLPWepNS6tvAeq31Pc57b1NKvQFEgeu11inW5MsMd7+8\nnzx/DueuSCN/erjsI3A78tFkH3nxTlIpneW6jLw1gawfP9lSGM2M5lR4LYVUvuwFp5tKnNZtU73Y\nFCmzlgOMnCWTDid+DF74pVtBNZn33DL+Y0xm4ku0dkqgOZtY95EEmdMi3eyjG7XWj6Z6Q2u9ZqgP\naa3vB+5Peu3rnuca+Lzzl1UGojH+9toBzjp62shBZhh+ngIYS6G3Lf1ieMl4LYWyWe6aBLZef47f\njWt4s3p8eW7WzpjdR2M6auoAABU3SURBVAGPKNiZ2x5RWHG5+bMkWAplg7cfK8ddPdhtNJWwAtvX\nJYHmbBLwxBSEEUnXfbTMGwBWSlUopa7LUpuywlM7WmjpCXPpcckJUEMwXEwB3GDzmN1HXkuhziza\nEx1wLYUapw5/8fTEHH5/wHUvjdlSKHQDzem4LWwGUqYthalO3FLoEvdRNpGYwqhIVxQ+orWOF8B3\n5hWMsmLUxNLQ3svMsgBnLEkzeyk6TPYRZEAUkiwFMP51Kwq22qLXdQSJNebHain4vZZCOqJgLYUy\nc0x/Qeb86lMZe877HFFQvsTMNiEzFFYay7tkkpTdmOSk6z7yKaWU4+6xs5UzsALG4ePqk+ZyxQmz\n8fvS1MFo2Ew0G7IevnPzjtUHHC/vrNyLNZwkCq/cOjj335v5NGb3UaFnNnEabovSWjPKsmmrgVKx\nFDKBPefWUsgvkZTJbJBXBB98KHF1PmFI0hWFBzFB5V84/3/Uee1NRdqCACamMJx7ZryWQn6JSevM\nK3Q7h3DIDTjb0sPJloK3TWN2H43SUlAKPvCAu3jNsVfAtOVjO7bgkuMz9aP6u01sR4LM2aNuyNCn\nkES6ovAljBB83Pl/HZBGzd03MdHI8DOGx5uSqpSJK/hy3ZhBuMeM4H35xlJYepGphe8lwVIYT0zB\niUv0dxvT2j9CpVfvWsPnfHtsxxUGEyh1A80STxAmAWmJgtY6BvzM+ZsaRCPDu2filsI4RndF1cZF\nFa9HFDKiUFBh0kHfc+vgz2QippBb4M6LELfFxJJf6qakiigIk4B05yksAv4TU+00PqTUWi8Y8kNv\ndqLh4Ttdf4Hp0MfjW199jXm0+wh7RGHI42bAUvAHIOJYCuEe6Ywmkril0J35aq+CMAbSdR/9GvgG\n8EPgTOADHOkL9MQGhu90S2ea4nDjGWGv+YB5PLTJPIZ7INQ+/CQbn6cO0FgK4oGxTCIhU5p7uHWB\nheyTX2rW1ejvmVwlPYQpS7ode4HW+u+A0lrv1Vp/E7gge82aBETDxtc+FG/5Anz475k5lo0pRNKx\nFBxRUL6x17bPDQDa1PDv75JMoonEaymIxSZMAtK1FPqdstnbndIV+4EjuyeJppF9lKmc8lwbaHZS\nUguGWd3KisJYXUfgxjAGeh23RfXw2wvZI7/ECHM4JKIgTArSHWp+BigEPg2sBq4G3p+tRk0KRgo0\nZxK7BF9cFNKwFMYjCt7V12SEOrHkl7qLKsnvIEwCRrQUnIlq79FafxHowcQTjnxih1EU/AWAMmWy\nB3qHr+ZoYwrjaZt39TURhYklUOrW2ZLfQZgEjGgpaK2jwGmHoS2Ti2h47IHc0ZLjpKXaonjZthRy\nxVKYNOR75rnI7yBMAtKNKbyslLoH+CMQXw1Ga/3nrLRqMhAdIfso0+QVuesgD1ejxZ9BSyEcNMFt\nyT6aOLzzXEQUhElAuqIQAFoB74onGjiCRSHs+voPB3mF0LrDuIfmDWOYZTKmEGwyj9IZTRz5IgrC\n5CLdGc1TI47gZaTaR5nGpoUedebwFUgzGVPosaJwZCeSTWoSLAWx2ISJJ90Zzb/GWAYJaK0/mPEW\nTRZGqn2UaWxHffSFw29nR/njEgUnlbb7oHmUEerEIZaCMMlI1330N8/zAHAZcCDzzZlEHM6UVDAx\nBZUzuABeMrb20XismPLZ5vHAy+ZROqOJw3vu5XcQJgHpuo/+5P1fKXU78FRWWjRZGKn2UaaZvdZU\nTS0aYSJZ3FIYhyjkl5iS3PXPO/+L22LCCEj2kTC5SNdSSGYRMC2TDZl0jFT7KNOc+ZX0trNtGq9g\nTV8Bm+8xz6Uzmji8517KjQiTgLRmNCulupVSXfYPuBezxsKRy0i1jyaKTFgKADNWus9FFCYOX66J\nJ+UVD73KnyAcRtJ1H029XmOk2kcTRSZiCmAsBYuIwsSSXyrrWQiThnQthcuUUmWe/8uVUpdmr1mT\ngMMdaE4XaymM14rxWgritphYAqUizMKkId2CeN/QWnfaf7TWHZj1FY5cDmfto9Hgy8DkNYCyOhPk\nzC0St8VEky+iIEwe0h1uphKPSehwzyCHs/bRaMjEjGYw7orpK80samFiWflO0LGJboUgAOl37OuV\nUj8Afur8/wngpew0aRIQi5qbdFLGFDIwo9my5gPQvGX8+xHGx0kfn+gWCEKcdEXhU8DXgD9gZjav\nwwjDkUnUKWXsm4TGUKYsBTAjVEEQBA/pZh8FgRuy3JbJg61vPxkthUzUPhIEQRiCdLOP1imlyj3/\nVyilHspesyYYaylM6pjCJGybIAhvetL1j1Q7GUcAaK3blVJH7ozmuPtoEna8SsEZX4HFb5volgiC\ncASSrijElFJztNb7AJRS80hRNfWIIRo2j5NRFADOOLInkwuCMHGkO0/hX4GnlFK3KKVuBR4HvjzS\nh5RS5yqltiqldiilhoxJKKUuV0pppdSaNNuTXSZzTEEQBCGLpCUKWusHgTXAVuB24AtA73CfUUr5\nMCms5wHLgCuVUstSbFcCfAZ4flQtzybxmMIkzD4SBEHIIukusvNhTMddB7wCnAQ8S+LynMmsBXZo\nrXc5+7gDuAR4I2m7/wd8F7h+VC3PJlGxFARBmJqk6z76DHACsFdrfSZwHNAx/EeYBdR7/m9wXouj\nlDoemK21vm+4HSmlrlVKrVdKrW9ubk6zyeNgsscUBEEQskS6otCnte4DUErla623AEvGc2ClVA7w\nA4wrali01r/UWq/RWq+pqakZz2HTIzZgHkUUBEGYYqTrNG9w5incDaxTSrUDe0f4zH5gtuf/Ouc1\nSwmwAnhMmbLBM4B7lFIXa63Xp9mu7GAthck4T0EQBCGLpDuj+TLn6TeVUo8CZcCDI3zsRWCRUmo+\nRgyuAK7y7LMTiK89qZR6DPjihAsCSExBEIQpy6jTa7TWj6e53YBS6pPAQ4APuFlrvUkp9W1gvdb6\nntEe+7AxmSevCYIgZJGs5lxqre8H7k967etDbHtGNtsyKmIiCoIgTE3SDTRPLSSmIAjCFEVEIRVR\nyT4SBGFqIqKQCpmnIAjCFEVEIRWxSVw6WxAEIYuIKKRCUlIFQZiiiCikYjIvxykIgpBFRBRSEY8p\niKUgCMLUQkQhFRJTEARhiiKikAqZ0SwIwhRFRCEV0YhZYMcU6hMEQZgyiCikIhqWeIIgCFMSEQUv\nXQfgd5dC90GJJwiCMCWRnEsv+zfArkehoELiCYIgTEnEUvASCZnH3nYRBUEQpiQiCl7CQfe5iIIg\nCFMQEQUv1lIAiSkIgjAlEVHwEvaIgmQfCYIwBRFR8BLxuo8kBi8IwtRDRMFLOGQmrYFYCoIgTElk\nOOwlEoSiGtBaYgqCIExJRBS8hEOQWwhLzpPsI0EQpiQiCl4iIcgrhLf/+0S3RBAEYUKQmIKXcBBy\niya6FYIgCBOGiIIXaykIgiBMUUQUvNiYgiAIwhRFRMFLJAh54j4SBGHqIqLgRSwFQRCmOCIKXiIh\nsRQEQZjSiChYtDaiIJaCIAhTGBEFS6TXPEr2kSAIU5isioJS6lyl1Fal1A6l1A0p3v+8UuoNpdRr\nSqm/K6XmZrM9w2LLZss8BUEQpjBZEwWllA/4KXAesAy4Uim1LGmzl4E1WutjgLuA/8pWe0bELrAj\nloIgCFOYbFoKa4EdWutdWuswcAdwiXcDrfWjWmu7iMFzQF0W2zM8cUtBREEQhKlLNkVhFlDv+b/B\neW0oPgQ8kOoNpdS1Sqn1Sqn1zc3NGWyiB7vAjmQfCYIwhZkUgWal1NXAGuC/U72vtf6l1nqN1npN\nTU1NdhphF9gRS0EQhClMNquk7gdme/6vc15LQCl1NvCvwOla6/4stmd44paCiIIgCFOXbFoKLwKL\nlFLzlVJ5wBXAPd4NlFLHAb8ALtZaN2WxLSMTtxTEfSQIwtQla6KgtR4APgk8BGwG7tRab1JKfVsp\ndbGz2X8DxcAflVKvKKXuGWJ32UcsBUEQhOwusqO1vh+4P+m1r3uen53N448KmacgCIIwOQLNkwKZ\npyAIgiCiECcSAhT4AxPdEkEQhAlDRMESdiqkKjXRLREEQZgwRBQskaDMURAEYcojomAJy/rMgiAI\nIgqWSEgyjwRBmPKIKFjCQbEUBEGY8ogoWGTVNUEQBBGFOOGgVEgVBGHKI6Jg6W2HgoqJboUgCMKE\nIqJgEVEQBEEQUQAg0mdiCoWVE90SQRCECUVEAYyVAGIpCIIw5RFRAOhtM48FYikIgjC1EVEACFlR\nEEtBEISpjYgCuO4jiSkIgjDFEVEAcR8JgiA4iCiAuI8EQRAcRBTAuI/8Aal9JAjClEdEAYz7SFxH\ngiAIIgoAhGQ2syAIAogoGHrbJfNIEAQBEQVDbxsUlE90KwRBECYcEQVwiuGJpSAIgiCioLVJSRX3\nkSAIgogC4R6IRSTQLAiCgIiCp0KqWAqCIAgiCjKbWRAEIY6Igq17JDEFQRAEEQU69pnHopqJbYcg\nCMIkQERh091QMQ+qFk50SwRBECacrIqCUupcpdRWpdQOpdQNKd7PV0r9wXn/eaXUvGy2ZxA9TbD7\ncVhxOSh1WA8tCIIwGcmaKCilfMBPgfOAZcCVSqllSZt9CGjXWi8Efgh8N1vtScmmu0HHYOW7Duth\nBUEQJiv+LO57LbBDa70LQCl1B3AJ8IZnm0uAbzrP7wJ+opRSWmud8dZsuAWe/Unia12NMG05TFua\n8cMJwv9v7/5jpDjrOI6/P4IltlSQFhtC+XFURGuigJfa2B8xqdGCtlRFRSvij4SYtInEGKVBq+l/\n1aiJSSPFlEgr2qa2xIupsRYNpn9QoHgUaEuhiCnkCoqGWrVV6Nc/nmenc8vtwV3dmYX9vJLNzj47\nt/fd7zw7352ZnWfMzkTtLApTgedKjw8C72k1T0Qcl3QMuAD4a3kmScuB5QDTp08fXTTnToLJcwa3\nTZ4D85aO7vXMzM5C7SwK/zcRsQZYA9Db2zu6rYi3fSjdzMyspXYeaD4ETCs9vji3DTmPpLHABOBo\nG2MyM7NhtLMobAVmS+qRdA6wBOhrmqcPWJanFwO/a8vxBDMzOy1t232UjxHcDPwGGAOsjYjdkm4D\ntkVEH3AXcI+kfcDfSIXDzMxq0tZjChHxEPBQU9utpemXAP8e1MysQ/iMZjMzK7gomJlZwUXBzMwK\nLgpmZlbQmfYLUEl/Af48yj+/kKazpTtIp8bmuEbGcY1cp8Z2tsU1IyJOeY2AM64ovBaStkVEb91x\nDKVTY3NcI+O4Rq5TY+vWuLz7yMzMCi4KZmZW6LaisKbuAIbRqbE5rpFxXCPXqbF1ZVxddUzBzMyG\n121bCmZmNgwXBTMzK3RNUZB0raQ9kvZJWlljHNMk/V7Sk5J2S/pybv+2pEOS+vNtYQ2xHZC0M///\nbbltkqTfStqb799UcUxzSjnpl/SCpBV15UvSWklHJO0qtQ2ZIyU/zH3uCUnzK47ru5Kezv97g6SJ\nuX2mpH+Xcre64rhaLjtJt+R87ZH0wXbFNUxs95XiOiCpP7dXkrNh1g/V9bGIOOtvpKG7nwVmAecA\nO4BLa4plCjA/T58PPANcSrpW9VdrztMB4MKmtu8AK/P0SuD2mpfj88CMuvIFXA3MB3adKkfAQuDX\ngIDLgccqjusDwNg8fXsprpnl+WrI15DLLn8OdgDjgJ78mR1TZWxNz38PuLXKnA2zfqisj3XLlsJl\nwL6I2B8R/wHuBRbVEUhEDETE9jz9D+Ap0rWqO9UiYF2eXgfcUGMs1wDPRsRoz2h/zSLiD6Rrf5S1\nytEi4O5INgMTJU2pKq6IeDgijueHm0lXP6xUi3y1sgi4NyJejog/AftIn93KY5Mk4BPAz9v1/1vE\n1Gr9UFkf65aiMBV4rvT4IB2wIpY0E5gHPJabbs6bgGur3k2TBfCwpMclLc9tF0XEQJ5+Hriohrga\nljD4Q1p3vhpa5aiT+t0XSN8oG3ok/VHSJklX1RDPUMuuk/J1FXA4IvaW2irNWdP6obI+1i1FoeNI\nGg88AKyIiBeAHwGXAHOBAdKma9WujIj5wALgJklXl5+MtL1ay2+YlS7pej1wf27qhHydpM4ctSJp\nFXAcWJ+bBoDpETEP+ArwM0lvrDCkjlx2TT7F4C8gleZsiPVDod19rFuKwiFgWunxxbmtFpJeT1rg\n6yPiQYCIOBwRJyLiFeDHtHGzuZWIOJTvjwAbcgyHG5uj+f5I1XFlC4DtEXE4x1h7vkpa5aj2fifp\nc8CHgRvzyoS8e+Zonn6ctO/+rVXFNMyyqz1fAJLGAh8F7mu0VZmzodYPVNjHuqUobAVmS+rJ3ziX\nAH11BJL3Vd4FPBUR3y+1l/cDfgTY1fy3bY7rPEnnN6ZJByl3kfK0LM+2DPhllXGVDPrmVne+mrTK\nUR/w2fwLkcuBY6VdAG0n6Vrga8D1EfGvUvtkSWPy9CxgNrC/wrhaLbs+YImkcZJ6clxbqoqr5P3A\n0xFxsNFQVc5arR+oso+1+2h6p9xIR+mfIVX4VTXGcSVp0+8JoD/fFgL3ADtzex8wpeK4ZpF++bED\n2N3IEXABsBHYCzwCTKohZ+cBR4EJpbZa8kUqTAPAf0n7b7/YKkekX4TckfvcTqC34rj2kfY3N/rZ\n6jzvx/Iy7ge2A9dVHFfLZQesyvnaAyyoelnm9p8AX2qat5KcDbN+qKyPeZgLMzMrdMvuIzMzOw0u\nCmZmVnBRMDOzgouCmZkVXBTMzKzgomBWIUnvk/SruuMwa8VFwczMCi4KZkOQ9BlJW/LY+XdKGiPp\nRUk/yOPcb5Q0Oc87V9JmvXrdgsZY92+R9IikHZK2S7okv/x4Sb9QutbB+nwWq1lHcFEwayLp7cAn\ngSsiYi5wAriRdGb1toh4B7AJ+Fb+k7uBr0fEO0lnlTba1wN3RMS7gPeSzp6FNPLlCtI4+bOAK9r+\npsxO09i6AzDrQNcA7wa25i/xbyANQPYKrw6S9lPgQUkTgIkRsSm3rwPuz+NITY2IDQAR8RJAfr0t\nkcfVUbqy10zg0fa/LbNTc1EwO5mAdRFxy6BG6ZtN8412jJiXS9Mn8OfQOoh3H5mdbCOwWNKbobg+\n7gzS52VxnufTwKMRcQz4e+miK0uBTZGumnVQ0g35NcZJOrfSd2E2Cv6GYtYkIp6U9A3SVeheRxpF\n8ybgn8Bl+bkjpOMOkIYyXp1X+vuBz+f2pcCdkm7Lr/HxCt+G2ah4lFSz0yTpxYgYX3ccZu3k3Udm\nZlbwloKZmRW8pWBmZgUXBTMzK7gomJlZwUXBzMwKLgpmZlb4H50VmgPJO9tJAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SP4EzHQiJlBo",
        "colab_type": "code",
        "outputId": "44389e47-61e1-4ee0-80bf-b81a3d62bfc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "\n",
        "preds = model.evaluate(X_test, Y_test)\n",
        "print (\"Loss = \" + str(preds[0]))\n",
        "print (\"Test Accuracy = \" + str(preds[1]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "504/504 [==============================] - 1s 2ms/step\n",
            "Loss = 1.1401852719367496\n",
            "Test Accuracy = 0.811507935561831\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEfOOXB8JlML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras_file = \"keras_model.h5\"\n",
        "model.save(keras_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgphcDfsJlXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2Vet_bUJlez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}